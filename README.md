## ðŸ“Œ Overview  
This project implements an Artificial Neural Network (ANN) from scratch using only NumPy. It covers the forward pass, backward pass, and training process without relying on deep learning frameworks like TensorFlow or PyTorch.  


## ðŸ“š Suggested Prerequisites
- Knowledge of linear algebra (matrix multiplication, dot product)  
- Understanding of Softmax regression and gradient descent with other optimizers
- Familiarity with NumPy for numerical computations  

## âœ… Key Topics Covered  
- Fully connected neural network implementation  
- Forward and backward propagation  
- Activation functions (ReLU, Softmax, etc.)  
- Gradient descent and other optimizers like ADAM, ADAGRAD, RMSPROP, and Momentum  
- Regularization techniques like dropout  
- Loss and accuracy visualization




## ðŸ“‚ Project Structure  
```
ANN_from_Scratch/  
â”‚-- Notebooks/  
â”‚   â”œâ”€â”€ ANN_from_scratch.ipynb  
â”‚-- Images/  
â”‚   â”œâ”€â”€ loss_plot.png  
â”‚   â”œâ”€â”€ accuracy.png
â”‚-- README.md
```



You can run the notebook step by step to understand how ANN works.



## ðŸ“Š Results & Visualization

### ðŸ”¹ Accuracy
The final accuracy of the ANN model with regularization drop out:

** Test Accuracy: 85 %**

### ðŸ”¹ Loss Reduction
The model's loss decreases over time, indicating successful learning.

![Loss Plot](Images/loss.png)

### ðŸ”¹ Accuracy plot
Validation accuracy is near to training accuracy, i.e model variance is low.

![Accuracy plot](Images/accuracy.png)




Acknowledgments

Inspired by "Neural Networks from Scratch" by Harrison Kinsley and Daniel KukieÅ‚a.
And Vizuara.ai
 